Сирий текст не підходить для навчання машин. Потрібно конвертувати текст у вектори – набори цифр. Цей процес називають виділенням ознак.

Мішок слів – техніка виділення ознак, яка описує входження слів у текст. 

Для використання моделі необхідно визначити словник відомих слів, які називають токенами, а також обрати ступінь їхньої присутності. 

#### Приклад

Розглянемо створення моделі, використовуючи приклад для наочності.

1.	Загрузка даних
   
   У нас є дані:
   I like this movie, it's funny.
   I hate this movie.
   This was awesome! I like it.
   Nice one. I love it.
   Загружаємо їх у вигляді масиву: ["I like this movie, it's funny.", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']

2.	Визначаємо словник

Збираємо усі унікальні слова, ігноруючи реєстр, пунктуацію, токени з одного символу.

3.	Створюємо вектори документа

Перетворюємо сирий текст у набір цифр, щоб використовувати їх як вхідні дані для моделі машинного навчання.
Відмічаємо наявність слів (1 – є, 0 – немає)
Результат:

|      | awesome | funny | hate | it   | like | love | movie | nice | one  | this | was  |
| ---: |--------:| -----:|-----:| ---: | ---: | ---: | ----: | ---: | ---: | ---: | ---: |
| 0    | 0       | 1     | 0    | 1    | 1    | 0    | 1     | 0    | 0    | 1    | 0    |
| 1    | 0       | 0     | 1    | 0    | 0    | 0    | 1     | 0    | 0    | 1    | 0    |
| 2    | 1       | 0     | 0    | 1    | 1    | 0    | 0     | 0    | 0    | 1    | 1    |
| 3    | 0       | 0     | 0    | 1    | 0    | 1    | 0     | 1    | 1    | 0    | 0    |

Із словником росте вектор документу. У прикладі довжина вектора дорівнює кількості відомих слів. Якщо вектор складається з тисяч або мільйонів елементів, кожен документ може містити лише меншу частину слів їх словника. Виходить вектор з багатьма нулями, який вимагає багато пам’яті та ресурсів.

Можна зменшити кількість слів, щоб зменшити вимоги до обчислювальних ресурсів.
Наприклад, викинути стоп-слова, привести слова до базових форм і виправити неправильно написані слова.
Інший спосіб – використання згрупованих слів, інакше кажучи, N-грам (N – кількість згрупованих слів). У модель потрапляють лише ті, що фігурують у корпусі.

#### Приклад:

Речення: The office building is open today

Його біграми (сполучення двох слів):
  * the office
  * office building
  * building is
  * is open
  * open today

Ми оцінили наявність слів, але також можна рахувати їх кількість у документі та частоту відносно загальної кількості.
 
 ### Сервіси, що реалізують NLP
 
 #### Textrazor
 
   В мережі існують певні сервіси, наприклад [textrazor.com](https://www.textrazor.com), який надає можливість безкоштовно протестувати можливості NLP (demo), та має кілька функціональних переваг. Подальшу інформацію було взято з <https://www.textrazor.com/technology>

* TextRazor використовує сучасні методи обробки природних мов та штучного інтелекту для аналізу, аналізу та вилучення семантичних метаданих із заданого вмісту.

* API TextRazor можна легко інтегрувати з будь-якою мовою, яка може надіслати HTTP-запит і проаналізувати відповідь JSON, завдяки чому можлива потужна аналітика тексту лише за допомогою декількох рядків коду. TextRazor дозволяє витягти будь-яку та всю необхідну інформацію в одному запиті, пов'язуючи витягнуті семантичні метадані, щоб спростити ідентифікацію складних шаблонів.

* Великі дані корисні лише в тому випадку, якщо користувацьке програмне забезпечення може йти в ногу з цим. TextRazor був розроблений з нуля для продуктивності. Написаний на сильно оптимізованому C ++, TextRazor здатний обробляти тисячі слів в секунду. Сервіс створений для автоматичного масштабування до мільярдів запитів у хмарі. 

* Стійка інфраструктура TextRazor побудована на хмарі Amazon Web Services і фізичному обладнанні. TextRazor розроблений для забезпечення високої доступності та послідовності продуктивності для аналізу тисяч, мільйонів або мільярдів щоденних документів.

* TextRazor дозволяє додавати назви продуктів, людей, компаній, спеціальні правила класифікації та вдосконалені мовні зразки. Інтегрований двигун Prolog дозволяє швидко поєднувати результати TextRazor з надійною логікою, призначеною для користувача. 

#### Наташа

Подальшу інформацію було взято з <https://habr.com/ru/post/349864/>

Для витягування слів з російського тексту рішень небагато. Є рішення у Томіта-парсері, але там інтеграція з Python незручна. Також з iPavlov є рішення, але імена не зводяться до нормальної форми. З витягуванням адрес або посилань на нормативні акти ще складніше.

Наташа – аналог Томіта-парсера для Python і набір готових правил для витягування імен, адрес, дат, сум грошей та інших сутностей. Можна додавати свої правила за допомогою Yargy-парсера.

Зараз є правила для витягування імен, адрес, дат і сум грошей.  Правила для назв організацій і географічних об’єктів нижчої якості. 

У 2016 році проводилося змагання factRuEval-2016 з витягування іменних сутностей. Найкраща F1-міра була більше 0.9. У Наташі результат 0.78 через проблеми з іноземними іменами та складними прізвищами. Для текстів з російськими іменами результат ~0.95.

Проблемою бібліотеки Наташа є обмеженість готовими правилами. Наприклад, не вдасться визначити ««1» липня 2018» як дату, бо в правилах не враховані лапки. Також не вдасться розібрати адресу без назви вулиці.
У таких випадках доводиться доповнювати готові правила та писати свої за допомогою Yargy-парсеру, бібліотеці, яка є основою Наташі.

Yargy-парсер – аналог яндексового Томіта-парсера для Python. Правила для витягування сутностей описуються за допомогою контекстно-вільних граматик і словників.
Граматики в Yargy записуються на спеціальному DSL-і. Застосовуються вони для таких задач як витягування дат в ISO-форматі («2018-03-04», «2014-04-28»).

У парсер вбудовано багато готових предикантів і є можливість додати свої. Для визначення морфології слів використовується pymorphy2.

Парсер також виконує інтерпрепацію, і, наприклад, замість «16 серпня» повертає Date(month=8, day=16). Результат роботи парсера – дерево розбору.
 
Для інтерпретації користувач розвішує на вузли дерева помітки.
 
У прикладі потрібно звести назви років, місяців і днів до числам. Ця процедура називається нормалізацією.
Наприклад, січень – 1, лютий – 2, березень – 3 і так далі.

Також у Yargy є механізм узгодження, наприклад, імен і прізвищ за родом, числом та відмінком.

Рішення Наташі під ліцензією MIT, тобто, можна вільно використовувати, модифікувати і так далі.

Недоліками Наташі є потреба вручну складати правила та повільна швидкість (наприклад, витягування імен у 10 разів повільніше ніж у Томіта-парсера). Також наявні помилки у стандартних правилах.

Адреса проекту на Github — <https://github.com/natasha>.

### ВЕСУМ-великий електронний словник української мови

Подальшу інформацію було взято з <https://r2u.org.ua/articles/vesum>

Бере початок з проекту ispell-uk, що його в 90-х роках створила група ентузіастів для перевірки орфографії української мови у відкритій ОС Linux. Багато років цей словник мав єдину функцію — перевіряти орфографію текстів. Але декілька років тому в інший відкритий проект, програму перевірки граматики та стилю LanguageTool, було додано модуль української мови

До проекту долучилася команда створення відкритого корпусу української мови БрУК

Для створення ВЕСУМ-а ми використали два найголовніших джерела: “Граматичний словник української літературної мови. Словозміна” (опублікований 2011 року й удоступнений на Лінгвістичному порталі (<http://www.mova.info/grmasl.aspx>) та “Словники України” онлайн (<http://lcorp.ulif.org.ua/dictua/>).

#### Зусилля, вкладені в проект, на виході дали без перебільшення унікальний словник:

Основні застосування:

  * налічує понад 285 тис. лем і постійно поповнюється

  * містить інформацію про відмінювання слів

  * подає нерекомендовані слова (активні дієприкметники, невдалі кальки тощо) та заміну для них

  * охоплює абревіатури та скорочення

  * містить інформацію про деякі альтернативні правописні варіанти (дає змогу аналізувати тексти, написані не за чинним правописом, адже низка медій, авторів, видавництв свідомо дотримуються альтернативних правописних правил)

  * має велику базу власних імен (зокрема українських імен, по батькові та прізвищ, неукраїнських імен та прізвищ, українських та закордонних топонімів тощо)

  * синхронізований з КОАТУУ, зокрема містить назви, що з’явилися внаслідок декомунізації

  * має дуже компактну систему позначення відмінювання та тегів для слів, завдяки чому легко додавати нові слова, групувати наявні тощо

  * містить інформацію про деякі рідкісні та розмовні форми, наприклад, нестягнені форми прикметників (гарная), та розмовні інфінітиви (поїхать); щоправда, більшість таких форм вимкнено за уставою, оскільки вони мають обмежену форму застосування й часто створюють зайву омонімію (однак за потреби їх можна ввімкнути)

  * є відкритим проектом (розміщений на github), тож кожен може долучитися до вдосконалення та використовувати його у своїй роботі.

Інші застосування

  * укладання тлумачних, термінологічних, перекладних та інших типів словників (зокрема пошук прикладів вживання)

  * різноманітні мовознавчі дослідження

  * дослідження і розробки у галузі комп’ютерної лінгвістики (зокрема побудова моделей мови, отримання статистичної інформації)

  * довідкові функції та редагування

#### Структура словника

Словник містить три основні частини:
  1. правила зміни суфіксів у парадигмах
  2. слова з прапорцями парадигм та додаткових властивостей
  3. код генерування повних парадигм із сирцевої інформації (1 та 2)

Наприклад, запис тракторист /n20.a.p. – означає, що це відмінюване слово чоловічого роду другої відміни без чергування (і/о), істота, з закінченням -а в родовому відмінку.

### Де застосовується NLP?

Наведена нижче інформація була взята з цього ресурсу : https://evergreens.com.ua/ru/articles/natural-language-processing.html

Якщо вам здається, що ви ніколи не стикалися з NLP, то досить відкрити Google, біля пошукового рядка натиснути на значок мікрофона і сказати: «Окей, гугл ...». І ось пошукова система обробляє потрібний вам запит.

Але ця функція була б недоступна, якби не можливість пристрою зрозуміти природну мову, якою розмовляють люди. Здатність машини обробляти сказане, структурувати отриману інформацію, визначати необхідну відповідь дія і відповідати на мові, зрозумілій користувачеві, і є NLP або Natural Language Processing.

#### Чатбот

NLP стало основою для створення чатботу. Слід сказати, що за допомогою чатботу вирішується проблема завантаженості колл-центрів і приймальних відділень. Наприклад, після впровадження компанією "Київстар" чатботу Зоряна, завантаження операторів значно знизилася. Завдяки великій базі в 12 000 стандартизованих відповідей, бот допомагає з рішенням 70% входять питань. Це підтверджує ефективність використання чатботу для великих компаній.

Багато організацій потребують NLP як в помічника для структурування даних. Адже на сьогодні ще існує завдання оцифровки інформації. І тут ми знову звертаємося до обробника природної мови, який аналізує документацію і класифікує її.Використовують NLP і в медицині для поліпшення обслуговування пацієнтів, ведення медичних карт і пошуку ключових термінів у фаховій літературі. З його допомогою реалізовані лікарі-роботи, які зіставляють симптоми хворих з відповідними діагнозами і відстежують перебіг хвороби.Більш того, можливості обробки даних і прогнозування дозволяють використовувати NLP для запобігання злочинів. Застосовуючи її, поліція може аналізувати злочинну діяльність, обчислювати кодові слова злочинців в рекламі і швидше реагувати щоб уникнути насильства і торгівлі людьми. І це, напевно, найбільш вражаюче застосування NLP на сьогодні.


